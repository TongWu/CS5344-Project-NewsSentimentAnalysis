{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPDPbI/K18XTXgutZeD7cFR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TongWu/CS5344-Project-NewsSentimentAnalysis/blob/main/Notebook/RF_RAndomSEarch_Colab_Use.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3tjYcZ_YMX_"
      },
      "outputs": [],
      "source": [
        "# !pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0ztJCfEua-II",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39ca98f8-bd36-41e4-b1fd-3b37598bf017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0"
      ],
      "metadata": {
        "id": "8O-jBSn0yq33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install \"numpy<2.0\" \"tensorflow==2.10\""
      ],
      "metadata": {
        "id": "ZE5AXdbNXT_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pandas transformers optuna scikit-learn"
      ],
      "metadata": {
        "id": "8P_YtcyPWgxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMj_cweycmZb",
        "outputId": "5df739ca-07ce-433f-f3aa-1f307cc33f73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.6 alembic-1.13.3 colorlog-6.9.0 optuna-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna-integration[tfkeras]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gABm1-EccxtU",
        "outputId": "f9ebf3d7-b4fc-41e0-dee5-9fc8bbcc85b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna-integration[tfkeras]\n",
            "  Downloading optuna_integration-4.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (from optuna-integration[tfkeras]) (4.0.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from optuna-integration[tfkeras]) (2.17.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (1.13.3)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (6.0.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (0.37.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna->optuna-integration[tfkeras]) (1.3.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->optuna-integration[tfkeras]) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->optuna-integration[tfkeras]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->optuna-integration[tfkeras]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->optuna-integration[tfkeras]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->optuna-integration[tfkeras]) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna->optuna-integration[tfkeras]) (3.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->optuna-integration[tfkeras]) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->optuna-integration[tfkeras]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->optuna-integration[tfkeras]) (3.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow->optuna-integration[tfkeras]) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (0.1.2)\n",
            "Downloading optuna_integration-4.0.0-py3-none-any.whl (96 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/96.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: optuna-integration\n",
            "Successfully installed optuna-integration-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Libraries and Set Up Environment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm  # Added tqdm for progress visualization\n",
        "import h5py  # Added h5py for saving data\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Initialize GPU settings if available\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"physical GPUs,\", len(logical_gpus), \"logical GPUs.\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "lZtP0CY5yb0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a2c28b-2ab3-4578-8a53-481f59b64dde"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 physical GPUs, 1 logical GPUs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the entire dataset (without chunking)\n",
        "# print(\"Loading the dataset...\")\n",
        "# data_path = \"/content/drive/MyDrive/ggg_sg.csv\"\n",
        "# usecols = ['DateTime', 'Title', 'DomainCountryCode', 'ContextualText', 'DocTone']\n",
        "# df = pd.read_csv(data_path, usecols=usecols)\n",
        "# df_filtered = df[['ContextualText', 'DocTone']].dropna(subset=['ContextualText', 'DocTone'])\n",
        "# df_filtered['DocTone'] = df_filtered['DocTone'].astype(float)\n",
        "# # Initialize variables to store label encoder classes\n",
        "# label_encoder = LabelEncoder()\n",
        "\n",
        "# # Compute quantiles\n",
        "# percentiles = [0.2, 0.4, 0.6, 0.8]\n",
        "# quantiles = df_filtered['DocTone'].quantile(percentiles)\n",
        "# print(\"DocTone quantile thresholds:\", quantiles.values)\n",
        "# q1, q2, q3, q4 = quantiles.values\n",
        "\n",
        "# # Function to label sentiment\n",
        "# def label_sentiment(score):\n",
        "#     if score <= q1:\n",
        "#         return 'Strongly Negative'\n",
        "#     elif q1 < score <= q2:\n",
        "#         return 'Negative'\n",
        "#     elif q2 < score <= q3:\n",
        "#         return 'Neutral'\n",
        "#     elif q3 < score <= q4:\n",
        "#         return 'Positive'\n",
        "#     else:\n",
        "#         return 'Strongly Positive'\n",
        "\n",
        "# # Apply the sentiment labeling\n",
        "# df_filtered['Sentiment'] = df_filtered['DocTone'].apply(label_sentiment)\n",
        "\n",
        "# # Encode sentiments\n",
        "# df_filtered['SentimentLabel'] = label_encoder.fit_transform(df_filtered['Sentiment'])\n",
        "# num_labels = len(label_encoder.classes_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO6i67y63mrb",
        "outputId": "779b047e-22a5-4061-e070-a61670cd5614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the dataset...\n",
            "DocTone quantile thresholds: [-2.58706468 -0.65502183  0.79928952  2.47191011]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(num_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4cBjF0obAdK",
        "outputId": "91f5d733-acdf-44b5-91ed-0309bad93eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# h5f.close()"
      ],
      "metadata": {
        "id": "Eew76HTU39-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 2: Load and Preprocess Data\n",
        "# # Prepare h5py file to store processed data\n",
        "# h5_file = 'processed_data.h5'\n",
        "# if os.path.exists(h5_file):\n",
        "#     os.remove(h5_file)\n",
        "\n",
        "# h5f = h5py.File(h5_file, 'w')\n",
        "\n",
        "# # We will need to determine the max_length for tokenization\n",
        "# max_length = 1024  # Adjust as needed\n",
        "\n",
        "# # Initialize tokenizer and model\n",
        "# model_name = 'roberta-base'\n",
        "# tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "# transformer_model = TFRobertaModel.from_pretrained(model_name)\n",
        "\n",
        "# # Create datasets in h5py file with maxshape set to None to allow resizing\n",
        "# input_ids_dataset = h5f.create_dataset('input_ids', shape=(0, max_length), maxshape=(None, max_length), dtype='int32')\n",
        "# attention_masks_dataset = h5f.create_dataset('attention_masks', shape=(0, max_length), maxshape=(None, max_length), dtype='int32')\n",
        "# labels_dataset = h5f.create_dataset('labels', shape=(0,), maxshape=(None,), dtype='int32')\n",
        "\n",
        "# # Tokenize the texts and save to h5py in chunks\n",
        "# print(\"Tokenizing texts and saving to h5py file in batches...\")\n",
        "# texts = df_filtered['ContextualText'].tolist()\n",
        "# labels = df_filtered['SentimentLabel'].values\n",
        "\n",
        "# batch_size = 2  # Adjust based on your memory capacity\n",
        "# total_samples = 0  # Keep track of the total number of samples processed\n",
        "\n",
        "# for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing Batches\"):\n",
        "#     texts_batch = texts[i:i+batch_size]\n",
        "#     labels_batch = labels[i:i+batch_size]\n",
        "#     encoded = tokenizer(\n",
        "#         texts_batch,\n",
        "#         add_special_tokens=True,\n",
        "#         max_length=max_length,\n",
        "#         padding='max_length',\n",
        "#         truncation=True,\n",
        "#         return_attention_mask=True,\n",
        "#         return_tensors='np'  # Return numpy arrays\n",
        "#     )\n",
        "#     input_ids = encoded['input_ids']\n",
        "#     attention_masks = encoded['attention_mask']\n",
        "\n",
        "#     batch_size_actual = input_ids.shape[0]  # In case the last batch is smaller\n",
        "\n",
        "#     # Resize datasets to accommodate new data\n",
        "#     input_ids_dataset.resize((total_samples + batch_size_actual, max_length))\n",
        "#     input_ids_dataset[total_samples:total_samples + batch_size_actual] = input_ids\n",
        "\n",
        "#     attention_masks_dataset.resize((total_samples + batch_size_actual, max_length))\n",
        "#     attention_masks_dataset[total_samples:total_samples + batch_size_actual] = attention_masks\n",
        "\n",
        "#     labels_dataset.resize((total_samples + batch_size_actual,))\n",
        "#     labels_dataset[total_samples:total_samples + batch_size_actual] = labels_batch\n",
        "\n",
        "#     total_samples += batch_size_actual  # Update total samples processed\n",
        "\n",
        "# # Close the h5py file\n",
        "# h5f.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "BQLXnxeIzMNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b803af55-d66c-4ea9-ed1a-06b24c8abe62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.embeddings.position_ids', 'lm_head.bias']\n",
            "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing texts and saving to h5py file in batches...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batches: 100%|████████████████████████████████████████████████| 4592653/4592653 [1:11:32<00:00, 1070.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp /content/drive/MyDrive/processed_data.h5 /content/"
      ],
      "metadata": {
        "id": "GuSrwDAgkMGL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 3: Load Processed Data from h5py File\n",
        "# print(\"Loading processed data from h5py file...\")\n",
        "# h5_file = '/content/processed_data.h5'\n",
        "# h5f = h5py.File(h5_file, 'r')\n",
        "# input_ids = np.array(h5f['input_ids'])\n",
        "# attention_masks = np.array(h5f['attention_masks'])\n",
        "# labels = h5f['labels']\n",
        "\n",
        "# # Convert labels to numpy array\n",
        "# labels = np.array(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSRUT-SizZs4",
        "outputId": "71c8a3ee-e271-4f76-8435-265e9ba3dbde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading processed data from h5py file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 4: Split Data into Train and Test Sets\n",
        "# print(\"Splitting data into train and test sets...\")\n",
        "# labels = np.array(labels)\n",
        "# X_train_ids, X_test_ids, X_train_masks, X_test_masks, y_train, y_test = train_test_split(\n",
        "#     input_ids,\n",
        "#     attention_masks,\n",
        "#     labels,\n",
        "#     test_size=0.2,\n",
        "#     random_state=42,\n",
        "#     stratify=labels\n",
        "# )\n"
      ],
      "metadata": {
        "id": "kV8A6I3u6WCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with h5py.File('roberta_train_test_data.h5', 'w') as f:\n",
        "#     f.create_dataset('X_train_ids', data=X_train_ids)\n",
        "#     f.create_dataset('X_test_ids', data=X_test_ids)\n",
        "#     f.create_dataset('X_train_masks', data=X_train_masks)\n",
        "#     f.create_dataset('X_test_masks', data=X_test_masks)\n",
        "#     f.create_dataset('y_train', data=y_train)\n",
        "#     f.create_dataset('y_test', data=y_test)"
      ],
      "metadata": {
        "id": "dBoeTi9-uj-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/roberta_train_test_data.h5 /content/"
      ],
      "metadata": {
        "id": "y2eZ6WeguoxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with h5py.File('/content/roberta_train_test_data.h5', 'r') as f:\n",
        "#     X_train_ids = f['X_train_ids'][:]\n",
        "#     X_test_ids = f['X_test_ids'][:]\n",
        "#     X_train_masks = f['X_train_masks'][:]\n",
        "#     X_test_masks = f['X_test_masks'][:]\n",
        "#     y_train = f['y_train'][:]\n",
        "#     y_test = f['y_test'][:]\n",
        "with h5py.File('roberta_train_data.h5', 'r') as train_file:\n",
        "    X_train_ids = train_file['X_train_ids'][:]\n",
        "    X_train_masks = train_file['X_train_masks'][:]\n",
        "    y_train = train_file['y_train'][:]\n",
        "\n",
        "with h5py.File('roberta_test_data.h5', 'r') as test_file:\n",
        "    X_test_ids = test_file['X_test_ids'][:]\n",
        "    X_test_masks = test_file['X_test_masks'][:]\n",
        "    y_test = test_file['y_test'][:]"
      ],
      "metadata": {
        "id": "WeEAR48bYaT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Compute Class Weights\n",
        "print(\"Computing class weights...\")\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "JkiBUkWIwE8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up mixed precision training\n",
        "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_policy(policy)"
      ],
      "metadata": {
        "id": "y6RsURq3uz-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Define Function to Build the Model\n",
        "def build_model(transformer_model, learning_rate, dropout_rate, dense_units):\n",
        "    input_ids_in = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
        "    input_masks_in = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "    # Get transformer outputs\n",
        "    outputs = transformer_model(input_ids_in, attention_mask=input_masks_in)\n",
        "    cls_token = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
        "\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(cls_token)\n",
        "    x = tf.keras.layers.Dense(dense_units, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    output = tf.keras.layers.Dense(num_labels, activation='softmax', dtype='float32')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs=output)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    optimizer = mixed_precision.LossScaleOptimizer(optimizer, loss_scale='dynamic')\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "OEf1xN-dfw5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Hyperparameter Optimization with Optuna\n",
        "def objective(trial):\n",
        "    # Clear session and collect garbage\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.3)\n",
        "    dense_units = trial.suggest_int('dense_units', 64, 128, step=32)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [1, 2, 4])  # Reduced batch size\n",
        "\n",
        "    model = build_model(transformer_model, learning_rate, dropout_rate, dense_units)\n",
        "\n",
        "    # Use data generators to optimize memory usage\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {'input_ids': X_train_ids, 'attention_mask': X_train_masks}, y_train))\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {'input_ids': X_test_ids, 'attention_mask': X_test_masks}, y_test))\n",
        "    val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "    epochs = 3\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=epochs,\n",
        "        class_weight=class_weight_dict,\n",
        "        callbacks=[TFKerasPruningCallback(trial, 'val_accuracy')],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    val_accuracy = max(history.history['val_accuracy'])\n",
        "\n",
        "    # Clean up\n",
        "    del model\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    return val_accuracy\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "model_name = 'roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "transformer_model = TFRobertaModel.from_pretrained(model_name)\n",
        "transformer_model.trainable = False  # Freeze all layers\n",
        "max_length = 256\n",
        "num_labels = 5\n",
        "print(\"Starting hyperparameter optimization with Optuna...\")\n",
        "n_trials = 10\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Initialize tqdm progress bar\n",
        "with tqdm(total=n_trials, desc=\"Optuna Trials\") as progress_bar:\n",
        "    study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "print(\"Best hyperparameters:\")\n",
        "print(study.best_params)"
      ],
      "metadata": {
        "id": "l0k-Pxz_g0wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Build and Train the Final Model with Best Hyperparameters\n",
        "best_params = study.best_params\n",
        "learning_rate = best_params['learning_rate']\n",
        "dropout_rate = best_params['dropout_rate']\n",
        "dense_units = best_params['dense_units']\n",
        "batch_size = best_params['batch_size']\n",
        "\n",
        "model = build_model(learning_rate, dropout_rate, dense_units)\n",
        "\n",
        "# Set up callbacks\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1)\n",
        "\n",
        "print(\"Training the final model...\")\n",
        "epochs = 5  # Adjust as needed\n",
        "history = model.fit(\n",
        "    [X_train_ids, X_train_masks],\n",
        "    y_train,\n",
        "    validation_data=([X_test_ids, X_test_masks], y_test),\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "model.save(\"saved_model\")\n",
        "model.save(\"final_model.h5\")"
      ],
      "metadata": {
        "id": "-oU_9dbcCafT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Evaluate the Transformer Model\n",
        "print(\"Evaluating the transformer model...\")\n",
        "y_pred_probs = model.predict([X_test_ids, X_test_masks], batch_size=batch_size)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Output performance metrics\n",
        "print(\"Transformer Model Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "print(\"Transformer Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Transformer Model Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Transformer Model Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Transformer Model F1 Score:\", f1_score(y_test, y_pred, average='weighted'))"
      ],
      "metadata": {
        "id": "CXglu4r1iuve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Extract Embeddings and Train Random Forest Classifier\n",
        "print(\"Extracting embeddings from the fine-tuned model...\")\n",
        "# Define a new model to output embeddings\n",
        "embedding_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-3].output)  # Output before the last dense layer\n",
        "\n",
        "print(\"Getting embeddings for training and test data...\")\n",
        "X_train_embeddings = embedding_model.predict([X_train_ids, X_train_masks], batch_size=batch_size)\n",
        "X_test_embeddings = embedding_model.predict([X_test_ids, X_test_masks], batch_size=batch_size)\n",
        "\n",
        "# Save embeddings to h5py file\n",
        "print(\"Saving embeddings to h5py file...\")\n",
        "embeddings_file = 'embeddings.h5'\n",
        "if os.path.exists(embeddings_file):\n",
        "    os.remove(embeddings_file)\n",
        "emb_h5f = h5py.File(embeddings_file, 'w')\n",
        "emb_h5f.create_dataset('X_train_embeddings', data=X_train_embeddings)\n",
        "emb_h5f.create_dataset('X_test_embeddings', data=X_test_embeddings)\n",
        "emb_h5f.close()\n"
      ],
      "metadata": {
        "id": "zZ9sSDK_qTzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Hyperparameter Tuning and Training Random Forest Classifier\n",
        "print(\"Starting hyperparameter tuning for Random Forest with Optuna...\")\n",
        "def rf_objective(trial):\n",
        "    n_estimators = trial.suggest_int('n_estimators', 100, 500, step=100)\n",
        "    max_depth = trial.suggest_int('max_depth', 5, 30, step=5)\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)\n",
        "    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2'])\n",
        "\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf.fit(X_train_embeddings, y_train)\n",
        "    y_pred_rf = rf.predict(X_test_embeddings)\n",
        "    return accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "rf_study = optuna.create_study(direction='maximize')\n",
        "rf_study.optimize(rf_objective, n_trials=10)\n",
        "\n",
        "print(\"Random Forest Best Hyperparameters:\")\n",
        "print(rf_study.best_params)\n",
        "\n",
        "# Train Random Forest with best parameters\n",
        "best_rf_params = rf_study.best_params\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=best_rf_params['n_estimators'],\n",
        "    max_depth=best_rf_params['max_depth'],\n",
        "    min_samples_split=best_rf_params['min_samples_split'],\n",
        "    min_samples_leaf=best_rf_params['min_samples_leaf'],\n",
        "    max_features=best_rf_params['max_features'],\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "print(\"Training Random Forest Classifier...\")\n",
        "rf.fit(X_train_embeddings, y_train)"
      ],
      "metadata": {
        "id": "-Pw7yZDYtCNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Evaluate the Random Forest Model\n",
        "print(\"Evaluating the Random Forest model...\")\n",
        "y_pred_rf = rf.predict(X_test_embeddings)\n",
        "\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=label_encoder.classes_))\n",
        "\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Random Forest Precision:\", precision_score(y_test, y_pred_rf, average='weighted'))\n",
        "print(\"Random Forest Recall:\", recall_score(y_test, y_pred_rf, average='weighted'))\n",
        "print(\"Random Forest F1 Score:\", f1_score(y_test, y_pred_rf, average='weighted'))\n",
        "\n",
        "# Close the h5py files\n",
        "h5f.close()"
      ],
      "metadata": {
        "id": "pVluh03wtEdZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}