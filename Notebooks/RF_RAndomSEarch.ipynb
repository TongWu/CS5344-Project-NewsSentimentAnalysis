{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"c3tjYcZ_YMX_"},"outputs":[],"source":["# !pip install pyspark"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29740,"status":"ok","timestamp":1730659764275,"user":{"displayName":"Tong Wu","userId":"04187078703020700412"},"user_tz":-480},"id":"0ztJCfEua-II","outputId":"35963226-2749-438a-9667-1b9958e6a427"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8O-jBSn0yq33"},"outputs":[],"source":["# conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZE5AXdbNXT_q"},"outputs":[],"source":["# !pip install \"numpy<2.0\" \"tensorflow==2.10\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8P_YtcyPWgxQ"},"outputs":[],"source":["# !pip install pandas transformers optuna scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3386,"status":"ok","timestamp":1730659695470,"user":{"displayName":"Tong Wu","userId":"04187078703020700412"},"user_tz":-480},"id":"IdhU3A5ZWq5E","outputId":"bb3c9d0a-2e19-4237-97e0-f1d8f46a0792"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.0.0)\n","Collecting optuna-integration[tfkeras]\n","  Downloading optuna_integration-4.0.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.3)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from optuna-integration[tfkeras]) (2.17.0)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.6)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (3.12.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (18.1.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (0.4.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (3.4.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (2.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.16.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.64.1)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (2.17.0)\n","Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (3.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->optuna-integration[tfkeras]) (0.44.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (13.9.3)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (0.13.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->optuna-integration[tfkeras]) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->optuna-integration[tfkeras]) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->optuna-integration[tfkeras]) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->optuna-integration[tfkeras]) (2024.8.30)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->optuna-integration[tfkeras]) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->optuna-integration[tfkeras]) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->optuna-integration[tfkeras]) (3.0.6)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (0.1.2)\n","Downloading optuna_integration-4.0.0-py3-none-any.whl (96 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: optuna-integration\n","Successfully installed optuna-integration-4.0.0\n"]}],"source":["!pip install optuna optuna-integration[tfkeras]"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2735,"status":"ok","timestamp":1730707493051,"user":{"displayName":"Tong Wu","userId":"04187078703020700412"},"user_tz":-480},"id":"lZtP0CY5yb0w","outputId":"4bb12f8d-77ce-4855-de9d-ac44e2cdb1fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["1 physical GPUs, 1 logical GPUs.\n"]}],"source":["# Step 1: Import Libraries and Set Up Environment\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","import tensorflow as tf\n","from transformers import RobertaTokenizer, TFRobertaModel\n","\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","import optuna\n","from optuna.integration import TFKerasPruningCallback\n","\n","from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n","\n","from sklearn.ensemble import RandomForestClassifier\n","\n","import os\n","import random\n","from tqdm import tqdm  # Added tqdm for progress visualization\n","import h5py  # Added h5py for saving data\n","\n","import gc\n","\n","from concurrent.futures import ThreadPoolExecutor\n","\n","# Initialize GPU settings if available\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        logical_gpus = tf.config.list_logical_devices('GPU')\n","        print(len(gpus), \"physical GPUs,\", len(logical_gpus), \"logical GPUs.\")\n","    except RuntimeError as e:\n","        print(e)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64459,"status":"ok","timestamp":1730707557537,"user":{"displayName":"Tong Wu","userId":"04187078703020700412"},"user_tz":-480},"id":"BO6i67y63mrb","outputId":"f49a7a96-8313-47f6-aea5-0578ac0760e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading the dataset...\n","DocTone quantile thresholds: [-2.58706468 -0.65502183  0.79928952  2.47191011]\n"]}],"source":["# Load the entire dataset (without chunking)\n","print(\"Loading the dataset...\")\n","data_path = \"./Downloads/ggg_sg.csv\"\n","usecols = ['DateTime', 'Title', 'DomainCountryCode', 'ContextualText', 'DocTone']\n","df = pd.read_csv(data_path, usecols=usecols)\n","df_filtered = df[['ContextualText', 'DocTone']].dropna(subset=['ContextualText', 'DocTone'])\n","df_filtered['DocTone'] = df_filtered['DocTone'].astype(float)\n","# Initialize variables to store label encoder classes\n","label_encoder = LabelEncoder()\n","\n","# Compute quantiles\n","percentiles = [0.2, 0.4, 0.6, 0.8]\n","quantiles = df_filtered['DocTone'].quantile(percentiles)\n","print(\"DocTone quantile thresholds:\", quantiles.values)\n","q1, q2, q3, q4 = quantiles.values\n","\n","# Function to label sentiment\n","def label_sentiment(score):\n","    if score <= q1:\n","        return 'Strongly Negative'\n","    elif q1 < score <= q2:\n","        return 'Negative'\n","    elif q2 < score <= q3:\n","        return 'Neutral'\n","    elif q3 < score <= q4:\n","        return 'Positive'\n","    else:\n","        return 'Strongly Positive'\n","\n","# Apply the sentiment labeling\n","df_filtered['Sentiment'] = df_filtered['DocTone'].apply(label_sentiment)\n","\n","# Encode sentiments\n","df_filtered['SentimentLabel'] = label_encoder.fit_transform(df_filtered['Sentiment'])\n","num_labels = len(label_encoder.classes_)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1730612869584,"user":{"displayName":"Tong Wu","userId":"04187078703020700412"},"user_tz":-480},"id":"D4cBjF0obAdK","outputId":"695b7e56-485c-4e8e-9183-9d0fd85fd7ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["5\n"]}],"source":["# print(num_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eew76HTU39-P"},"outputs":[],"source":["# h5f.close()"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3783221,"status":"ok","timestamp":1730711340759,"user":{"displayName":"Tong Wu","userId":"04187078703020700412"},"user_tz":-480},"id":"BQLXnxeIzMNu","outputId":"34b214e7-3ee9-4afe-979c-8f1ed0a1250f"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.embeddings.position_ids', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Tokenizing texts and saving to h5py file in batches...\n"]},{"output_type":"stream","name":"stderr","text":["Processing Batches: 100%|████████████████████████████████████████████████| 4592653/4592653 [1:03:01<00:00, 1214.50it/s]\n"]}],"source":["# Step 2: Load and Preprocess Data\n","# Prepare h5py file to store processed data\n","h5_file = 'processed_data.h5'\n","if os.path.exists(h5_file):\n","    os.remove(h5_file)\n","\n","h5f = h5py.File(h5_file, 'w')\n","\n","# We will need to determine the max_length for tokenization\n","max_length = 256  # Adjust as needed\n","\n","# Initialize tokenizer and model\n","model_name = 'roberta-base'\n","tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","transformer_model = TFRobertaModel.from_pretrained(model_name)\n","\n","# Create datasets in h5py file with maxshape set to None to allow resizing\n","input_ids_dataset = h5f.create_dataset('input_ids', shape=(0, max_length), maxshape=(None, max_length), dtype='int32')\n","attention_masks_dataset = h5f.create_dataset('attention_masks', shape=(0, max_length), maxshape=(None, max_length), dtype='int32')\n","labels_dataset = h5f.create_dataset('labels', shape=(0,), maxshape=(None,), dtype='int32')\n","\n","# Tokenize the texts and save to h5py in chunks\n","print(\"Tokenizing texts and saving to h5py file in batches...\")\n","texts = df_filtered['ContextualText'].tolist()\n","labels = df_filtered['SentimentLabel'].values\n","\n","batch_size = 2  # Adjust based on your memory capacity\n","total_samples = 0  # Keep track of the total number of samples processed\n","\n","for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing Batches\"):\n","    texts_batch = texts[i:i+batch_size]\n","    labels_batch = labels[i:i+batch_size]\n","    encoded = tokenizer(\n","        texts_batch,\n","        add_special_tokens=True,\n","        max_length=max_length,\n","        padding='max_length',\n","        truncation=True,\n","        return_attention_mask=True,\n","        return_tensors='np'  # Return numpy arrays\n","    )\n","    input_ids = encoded['input_ids']\n","    attention_masks = encoded['attention_mask']\n","\n","    batch_size_actual = input_ids.shape[0]  # In case the last batch is smaller\n","\n","    # Resize datasets to accommodate new data\n","    input_ids_dataset.resize((total_samples + batch_size_actual, max_length))\n","    input_ids_dataset[total_samples:total_samples + batch_size_actual] = input_ids\n","\n","    attention_masks_dataset.resize((total_samples + batch_size_actual, max_length))\n","    attention_masks_dataset[total_samples:total_samples + batch_size_actual] = attention_masks\n","\n","    labels_dataset.resize((total_samples + batch_size_actual,))\n","    labels_dataset[total_samples:total_samples + batch_size_actual] = labels_batch\n","\n","    total_samples += batch_size_actual  # Update total samples processed\n","\n","# Close the h5py file\n","h5f.close()\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27870,"status":"ok","timestamp":1730711369662,"user":{"displayName":"Tong Wu","userId":"04187078703020700412"},"user_tz":-480},"id":"nSRUT-SizZs4","outputId":"6c70a9bf-4727-4e91-b514-caaf11779bd5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading processed data from h5py file...\n","Splitting data into train and test sets...\n"]}],"source":["# Step 3: Load Processed Data from h5py File\n","print(\"Loading processed data from h5py file...\")\n","h5_file = 'processed_data.h5'\n","h5f = h5py.File(h5_file, 'r')\n","input_ids = np.array(h5f['input_ids'])\n","attention_masks = np.array(h5f['attention_masks'])\n","labels = h5f['labels']\n","\n","# Convert labels to numpy array\n","labels = np.array(labels)\n","\n","# Step 4: Split Data into Train and Test Sets\n","print(\"Splitting data into train and test sets...\")\n","labels = np.array(labels)\n","X_train_ids, X_test_ids, X_train_masks, X_test_masks, y_train, y_test = train_test_split(\n","    input_ids,\n","    attention_masks,\n","    labels,\n","    test_size=0.3,\n","    random_state=42,\n","    stratify=labels\n",")\n","X_train_ids = np.concatenate([X_train_ids, X_test_ids])\n","X_train_masks = np.concatenate([X_train_masks, X_test_masks])\n","y_train = np.concatenate([y_train, y_test])"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"1UEDS5K4sSy4","executionInfo":{"status":"ok","timestamp":1730711421954,"user_tz":-480,"elapsed":52289,"user":{"displayName":"Tong Wu","userId":"04187078703020700412"}}},"outputs":[],"source":["# Save in one shot\n","with h5py.File('roberta_train_test_data.h5', 'w') as f:\n","    f.create_dataset('X_train_ids', data=X_train_ids)\n","    f.create_dataset('X_test_ids', data=X_test_ids)\n","    f.create_dataset('X_train_masks', data=X_train_masks)\n","    f.create_dataset('X_test_masks', data=X_test_masks)\n","    f.create_dataset('y_train', data=y_train)\n","    f.create_dataset('y_test', data=y_test)\n","\n","# Save by train/test\n","with h5py.File('roberta_train_data.h5', 'w') as train_file:\n","    train_file.create_dataset('X_train_ids', data=X_train_ids)\n","    train_file.create_dataset('X_train_masks', data=X_train_masks)\n","    train_file.create_dataset('y_train', data=y_train)\n","\n","with h5py.File('roberta_test_data.h5', 'w') as test_file:\n","    test_file.create_dataset('X_test_ids', data=X_test_ids)\n","    test_file.create_dataset('X_test_masks', data=X_test_masks)\n","    test_file.create_dataset('y_test', data=y_test)\n","\n","# Save individually\n","with h5py.File('roberta_X_train_ids.h5', 'w') as f:\n","    f.create_dataset('X_train_ids', data=X_train_ids)\n","\n","with h5py.File('roberta_X_test_ids.h5', 'w') as f:\n","    f.create_dataset('X_test_ids', data=X_test_ids)\n","\n","with h5py.File('roberta_X_train_masks.h5', 'w') as f:\n","    f.create_dataset('X_train_masks', data=X_train_masks)\n","\n","with h5py.File('roberta_X_test_masks.h5', 'w') as f:\n","    f.create_dataset('X_test_masks', data=X_test_masks)\n","\n","with h5py.File('roberta_y_train.h5', 'w') as f:\n","    f.create_dataset('y_train', data=y_train)\n","\n","with h5py.File('roberta_y_test.h5', 'w') as f:\n","    f.create_dataset('y_test', data=y_test)"]},{"cell_type":"code","source":["from transformers import TFAutoModel, RobertaTokenizer\n","model_name = 'roberta-base'\n","tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","transformer_model = TFAutoModel.from_pretrained(model_name)\n","transformer_model.trainable = False\n","\n","class EmbeddingExtractor(tf.keras.layers.Layer):\n","    def __init__(self, transformer_model, **kwargs):\n","        super(EmbeddingExtractor, self).__init__(**kwargs)\n","        self.transformer = transformer_model\n","\n","    def call(self, inputs):\n","        input_ids, attention_mask = inputs\n","        outputs = self.transformer(input_ids, attention_mask=attention_mask)\n","        return outputs.last_hidden_state[:, 0, :]  # 提取 [CLS] token 的嵌入\n","\n","input_ids_in = tf.keras.Input(shape=(256,), dtype=tf.int32, name='input_ids')\n","input_masks_in = tf.keras.Input(shape=(256,), dtype=tf.int32, name='attention_mask')\n","\n","# 使用自定义的嵌入提取器\n","embedding_extractor = EmbeddingExtractor(transformer_model)\n","embeddings = embedding_extractor([input_ids_in, input_masks_in])\n","embedding_model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs=embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rocy0t9aVBAa","executionInfo":{"status":"ok","timestamp":1730711426921,"user_tz":-480,"elapsed":4957,"user":{"displayName":"Tong Wu","userId":"04187078703020700412"}},"outputId":"e7d4f066-bce0-4e49-ed9d-3ecdf6bece31"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.embeddings.position_ids', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["print(\"Extracting embeddings for training and test data...\")\n","X_train_embeddings = embedding_model.predict([X_train_ids, X_train_masks], batch_size=16)\n","X_test_embeddings = embedding_model.predict([X_test_ids, X_test_masks], batch_size=16)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DluJJUmiVDfL","executionInfo":{"status":"error","timestamp":1730712561484,"user_tz":-480,"elapsed":5356,"user":{"displayName":"Tong Wu","userId":"04187078703020700412"}},"outputId":"6ab1583a-8ff7-4896-fdb5-eb2c6b5f4f6c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracting embeddings for training and test data...\n","    93/574082 [..............................] - ETA: 8:37:26"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting embeddings for training and test data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m X_train_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_masks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m X_test_embeddings \u001b[38;5;241m=\u001b[39m embedding_model\u001b[38;5;241m.\u001b[39mpredict([X_test_ids, X_test_masks], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n","File \u001b[1;32mD:\\anaconda3\\envs\\ww\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[1;32mD:\\anaconda3\\envs\\ww\\lib\\site-packages\\keras\\engine\\training.py:2253\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   2252\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2253\u001b[0m     tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   2255\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[1;32mD:\\anaconda3\\envs\\ww\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[1;32mD:\\anaconda3\\envs\\ww\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[1;32mD:\\anaconda3\\envs\\ww\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mD:\\anaconda3\\envs\\ww\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mD:\\anaconda3\\envs\\ww\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[1;32mD:\\anaconda3\\envs\\ww\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n","File \u001b[1;32mD:\\anaconda3\\envs\\ww\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HhCF5QRGW18e"},"outputs":[],"source":["!cp -r \"/content/drive/MyDrive/CS5344 Project Data/max_length(token)512/Test Train Split/Each/\"* /content/"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7847,"status":"ok","timestamp":1730660672518,"user":{"displayName":"Tong Wu","userId":"04187078703020700412"},"user_tz":-480},"id":"TKOWznU1_GG1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1c0deefe-6c49-43ed-d29a-253bd9b125d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n","Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9\n"]}],"source":["# Set up mixed precision training\n","from tensorflow.keras import mixed_precision\n","mixed_precision.set_global_policy('mixed_float16')\n","\n","max_length = 256\n","num_labels = 5\n","\n","# Open H5 files (after reprocessing data with max_length=256)\n","# train_ids_file = h5py.File('roberta_X_train_ids.h5', 'r')\n","# X_train_ids = train_ids_file['X_train_ids']\n","\n","# train_masks_file = h5py.File('roberta_X_train_masks.h5', 'r')\n","# X_train_masks = train_masks_file['X_train_masks']\n","\n","# y_train_file = h5py.File('roberta_y_train.h5', 'r')\n","# y_train = y_train_file['y_train']\n","\n","# test_ids_file = h5py.File('roberta_X_test_ids.h5', 'r')\n","# X_test_ids = test_ids_file['X_test_ids']\n","\n","# test_masks_file = h5py.File('roberta_X_test_masks.h5', 'r')\n","# X_test_masks = test_masks_file['X_test_masks']\n","\n","# y_test_file = h5py.File('roberta_y_test.h5', 'r')\n","# y_test = y_test_file['y_test']\n","\n","with h5py.File('./256/roberta_X_train_ids.h5', 'r') as f:\n","    X_train_ids = f['X_train_ids'][:]\n","\n","with h5py.File('./256/roberta_X_test_ids.h5', 'r') as f:\n","    X_test_ids = f['X_test_ids'][:]\n","\n","with h5py.File('./256/roberta_X_train_masks.h5', 'r') as f:\n","    X_train_masks = f['X_train_masks'][:]\n","\n","with h5py.File('./256/roberta_X_test_masks.h5', 'r') as f:\n","    X_test_masks = f['X_test_masks'][:]\n","\n","with h5py.File('./256/roberta_y_train.h5', 'r') as f:\n","    y_train = f['y_train'][:]\n","\n","with h5py.File('./256/roberta_y_test.h5', 'r') as f:\n","    y_test = f['y_test'][:]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":601,"status":"ok","timestamp":1730660673126,"user":{"displayName":"Tong Wu","userId":"04187078703020700412"},"user_tz":-480},"id":"JkiBUkWIwE8h","outputId":"eb2661b0-7151-42ab-d139-f8e3305141b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Computing class weights...\n"]}],"source":["# Step 5: Compute Class Weights\n","print(\"Computing class weights...\")\n","labels = y_train[:]\n","class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n","class_weight_dict = dict(enumerate(class_weights))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OEf1xN-dfw5C"},"outputs":[],"source":["# # Step 6: Define Function to Build the Model\n","# from transformers import TFAutoModel, RobertaTokenizer\n","\n","# # Define the model\n","# class TransformerLayer(tf.keras.layers.Layer):\n","#     def __init__(self, transformer_model, **kwargs):\n","#         super(TransformerLayer, self).__init__(**kwargs)\n","#         self.transformer = transformer_model\n","\n","#     def call(self, inputs):\n","#         input_ids, attention_mask = inputs\n","#         outputs = self.transformer(input_ids, attention_mask=attention_mask)\n","#         return outputs.last_hidden_state\n","\n","# def build_model(transformer_model, learning_rate, dropout_rate, dense_units):\n","#     input_ids_in = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n","#     input_masks_in = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n","\n","#     # Use the custom Transformer layer\n","#     transformer_layer = TransformerLayer(transformer_model)\n","#     transformer_outputs = transformer_layer([input_ids_in, input_masks_in])\n","\n","#     cls_token = transformer_outputs[:, 0, :]  # [CLS] token\n","\n","#     x = tf.keras.layers.Dropout(dropout_rate)(cls_token)\n","#     x = tf.keras.layers.Dense(dense_units, activation='relu')(x)\n","#     x = tf.keras.layers.Dropout(dropout_rate)(x)\n","#     output = tf.keras.layers.Dense(num_labels, activation='softmax', dtype='float32')(x)\n","\n","#     model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs=output)\n","#     optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","#     optimizer = mixed_precision.LossScaleOptimizer(optimizer, dynamic=True)\n","#     model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","#     return model\n","\n","# # Define the data generator without shuffling indices\n","# def data_generator(ids_dataset, masks_dataset, labels_dataset, batch_size):\n","#     dataset_size = labels_dataset.shape[0]\n","\n","#     for start_idx in range(0, dataset_size, batch_size):\n","#         end_idx = min(start_idx + batch_size, dataset_size)\n","#         batch_input_ids = ids_dataset[start_idx:end_idx]\n","#         batch_attention_masks = masks_dataset[start_idx:end_idx]\n","#         batch_labels = labels_dataset[start_idx:end_idx]\n","\n","#         yield ({'input_ids': batch_input_ids, 'attention_mask': batch_attention_masks}, batch_labels)\n","\n","# # Define the objective function for Optuna\n","# def objective(trial, X_train_ids):\n","#     # Clear session and collect garbage\n","#     tf.keras.backend.clear_session()\n","#     gc.collect()\n","\n","#     learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True)\n","#     dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.3)\n","#     dense_units = trial.suggest_int('dense_units', 64, 128, step=32)\n","#     batch_size = trial.suggest_categorical('batch_size', [1, 2, 4])\n","\n","#     model = build_model(transformer_model, learning_rate, dropout_rate, dense_units)\n","\n","#     # Recreate datasets with the new batch_size\n","#     train_dataset = tf.data.Dataset.from_generator(\n","#         lambda: data_generator(X_train_ids, X_train_masks, y_train, batch_size),\n","#         output_types=({'input_ids': tf.int32, 'attention_mask': tf.int32}, tf.int32),\n","#         output_shapes=(\n","#             {'input_ids': (None, max_length), 'attention_mask': (None, max_length)},\n","#             (None,)\n","#         )\n","#     )\n","#     # Shuffle the dataset\n","#     train_dataset = train_dataset.shuffle(buffer_size=10000).prefetch(tf.data.AUTOTUNE)\n","\n","#     val_dataset = tf.data.Dataset.from_generator(\n","#         lambda: data_generator(X_test_ids, X_test_masks, y_test, batch_size),\n","#         output_types=({'input_ids': tf.int32, 'attention_mask': tf.int32}, tf.int32),\n","#         output_shapes=(\n","#             {'input_ids': (None, max_length), 'attention_mask': (None, max_length)},\n","#             (None,)\n","#         )\n","#     )\n","#     val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n","\n","#     epochs = 3\n","#     history = model.fit(\n","#         train_dataset,\n","#         validation_data=val_dataset,\n","#         epochs=epochs,\n","#         steps_per_epoch=len(X_train_ids) // batch_size // 20,\n","#         validation_steps=len(X_test_ids) // batch_size // 20,\n","#         class_weight=class_weight_dict,\n","#         callbacks=[TFKerasPruningCallback(trial, 'val_accuracy')],\n","#         verbose=1\n","#     )\n","\n","#     val_accuracy = max(history.history['val_accuracy'])\n","\n","#     # Clean up\n","#     del model\n","#     tf.keras.backend.clear_session()\n","#     gc.collect()\n","\n","#     return val_accuracy\n","\n","# # Initialize tokenizer and model\n","# model_name = 'roberta-base'\n","# tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","# transformer_model = TFAutoModel.from_pretrained(model_name)\n","# transformer_model.trainable = False  # Freeze Transformer layers\n","\n","# print(\"Starting hyperparameter optimization with Optuna...\")\n","# n_trials = 10\n","# study = optuna.create_study(direction='maximize')\n","\n","# # Initialize tqdm progress bar\n","# with tqdm(total=n_trials, desc=\"Optuna Trials\") as progress_bar:\n","#     def objective_with_progress(trial):\n","#         result = objective(trial)\n","#         progress_bar.update(1)\n","#         return result\n","\n","#     study.optimize(objective_with_progress, n_trials=n_trials)\n","\n","# print(\"Best hyperparameters:\")\n","# print(study.best_params)\n","\n","# # Close H5 files after training\n","# train_ids_file.close()\n","# train_masks_file.close()\n","# y_train_file.close()\n","# test_ids_file.close()\n","# test_masks_file.close()\n","# y_test_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-oU_9dbcCafT","outputId":"eda7f4cd-c888-4092-dc36-57307d5fff12"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.bias', 'roberta.embeddings.position_ids', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Training the final model...\n","Epoch 1/5\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n","  7892/183706 [>.............................] - ETA: 3:45:15 - loss: 1.2293 - accuracy: 0.4553"]}],"source":["# Step 8: Build and Train the Final Model with Best Hyperparameters\n","from transformers import TFAutoModel, RobertaTokenizer\n","# best_params = study.best_params\n","learning_rate = 4e-5 #best_params['learning_rate']\n","dropout_rate = 0.2 #best_params['dropout_rate']\n","dense_units = 128 #best_params['dense_units']\n","batch_size = 8 #best_params['batch_size']\n","epochs = 5\n","\n","model_name = 'roberta-base'\n","tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","transformer_model = TFAutoModel.from_pretrained(model_name)\n","\n","from transformers import TFAutoModel, RobertaTokenizer\n","\n","# Define the model\n","class TransformerLayer(tf.keras.layers.Layer):\n","    def __init__(self, transformer_model, **kwargs):\n","        super(TransformerLayer, self).__init__(**kwargs)\n","        self.transformer = transformer_model\n","\n","    def call(self, inputs):\n","        input_ids, attention_mask = inputs\n","        outputs = self.transformer(input_ids, attention_mask=attention_mask)\n","        return outputs.last_hidden_state\n","\n","def build_model(transformer_model, learning_rate, dropout_rate, dense_units):\n","    input_ids_in = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n","    input_masks_in = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n","\n","    # Use the custom Transformer layer\n","    transformer_layer = TransformerLayer(transformer_model)\n","    transformer_outputs = transformer_layer([input_ids_in, input_masks_in])\n","\n","    cls_token = transformer_outputs[:, 0, :]  # [CLS] token\n","\n","    x = tf.keras.layers.Dropout(dropout_rate)(cls_token)\n","    x = tf.keras.layers.Dense(dense_units, activation='relu')(x)\n","    x = tf.keras.layers.Dropout(dropout_rate)(x)\n","    output = tf.keras.layers.Dense(num_labels, activation='softmax', dtype='float32')(x)\n","\n","    model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs=output)\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    optimizer = mixed_precision.LossScaleOptimizer(optimizer, dynamic=True)\n","    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","model = build_model(transformer_model, learning_rate, dropout_rate, dense_units)\n","\n","# Set up callbacks\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1)\n","\n","print(\"Training the final model...\")\n","\n","def get_data_subset(ids_dataset, masks_dataset, labels_dataset, epoch, total_epochs):\n","    dataset_size = labels_dataset.shape[0]\n","    start_idx = epoch * dataset_size // total_epochs\n","    end_idx = (epoch + 1) * dataset_size // total_epochs\n","    return (\n","        ids_dataset[start_idx:end_idx],\n","        masks_dataset[start_idx:end_idx],\n","        labels_dataset[start_idx:end_idx]\n","    )\n","\n","for epoch in range(epochs):\n","    print(f\"Epoch {epoch + 1}/{epochs}\")\n","\n","    # Get subset of training data for current epoch\n","    X_train_ids_epoch, X_train_masks_epoch, y_train_epoch = get_data_subset(\n","        X_train_ids, X_train_masks, y_train, epoch, epochs\n","    )\n","\n","    # Get subset of validation data for current epoch\n","    X_test_ids_epoch, X_test_masks_epoch, y_test_epoch = get_data_subset(\n","        X_test_ids, X_test_masks, y_test, epoch, epochs\n","    )\n","\n","    # Create training and validation dataset\n","    train_dataset = tf.data.Dataset.from_tensor_slices((\n","        {'input_ids': X_train_ids_epoch, 'attention_mask': X_train_masks_epoch},\n","        y_train_epoch\n","    )).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","    val_dataset = tf.data.Dataset.from_tensor_slices((\n","        {'input_ids': X_test_ids_epoch, 'attention_mask': X_test_masks_epoch},\n","        y_test_epoch\n","    )).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","    # Train model\n","    history = model.fit(\n","        train_dataset,\n","        validation_data=val_dataset,\n","        epochs=1,\n","        class_weight=class_weight_dict,\n","        callbacks=[early_stopping, reduce_lr]\n","    )\n","\n","    # Early Stopping\n","    if early_stopping.stopped_epoch > 0:\n","        print(\"Stop Training Early\")\n","        break\n","model.save(\"saved_model\")\n","model.save(\"final_model.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXglu4r1iuve"},"outputs":[],"source":["# Step 9: Evaluate the Transformer Model\n","print(\"Evaluating the transformer model...\")\n","y_pred_probs = model.predict([X_test_ids, X_test_masks], batch_size=batch_size)\n","y_pred = np.argmax(y_pred_probs, axis=1)\n","\n","# Output performance metrics\n","print(\"Transformer Model Classification Report:\")\n","print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n","\n","print(\"Transformer Model Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"Transformer Model Precision:\", precision_score(y_test, y_pred, average='weighted'))\n","print(\"Transformer Model Recall:\", recall_score(y_test, y_pred, average='weighted'))\n","print(\"Transformer Model F1 Score:\", f1_score(y_test, y_pred, average='weighted'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZ9sSDK_qTzR"},"outputs":[],"source":["# # Step 10: Extract Embeddings and Train Random Forest Classifier\n","# print(\"Extracting embeddings from the fine-tuned model...\")\n","# # Define a new model to output embeddings\n","# embedding_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-3].output)  # Output before the last dense layer\n","\n","# print(\"Getting embeddings for training and test data...\")\n","# X_train_embeddings = embedding_model.predict([X_train_ids, X_train_masks], batch_size=batch_size)\n","# X_test_embeddings = embedding_model.predict([X_test_ids, X_test_masks], batch_size=batch_size)\n","\n","# # Save embeddings to h5py file\n","# print(\"Saving embeddings to h5py file...\")\n","# embeddings_file = 'embeddings.h5'\n","# if os.path.exists(embeddings_file):\n","#     os.remove(embeddings_file)\n","# emb_h5f = h5py.File(embeddings_file, 'w')\n","# emb_h5f.create_dataset('X_train_embeddings', data=X_train_embeddings)\n","# emb_h5f.create_dataset('X_test_embeddings', data=X_test_embeddings)\n","# emb_h5f.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Pw7yZDYtCNQ"},"outputs":[],"source":["# # Step 11: Hyperparameter Tuning and Training Random Forest Classifier\n","# print(\"Starting hyperparameter tuning for Random Forest with Optuna...\")\n","# def rf_objective(trial):\n","#     n_estimators = trial.suggest_int('n_estimators', 100, 500, step=100)\n","#     max_depth = trial.suggest_int('max_depth', 5, 30, step=5)\n","#     min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n","#     min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)\n","#     max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2'])\n","\n","#     rf = RandomForestClassifier(\n","#         n_estimators=n_estimators,\n","#         max_depth=max_depth,\n","#         min_samples_split=min_samples_split,\n","#         min_samples_leaf=min_samples_leaf,\n","#         max_features=max_features,\n","#         random_state=42,\n","#         n_jobs=-1\n","#     )\n","#     rf.fit(X_train_embeddings, y_train)\n","#     y_pred_rf = rf.predict(X_test_embeddings)\n","#     return accuracy_score(y_test, y_pred_rf)\n","\n","# rf_study = optuna.create_study(direction='maximize')\n","# rf_study.optimize(rf_objective, n_trials=10)\n","\n","# print(\"Random Forest Best Hyperparameters:\")\n","# print(rf_study.best_params)\n","\n","# # Train Random Forest with best parameters\n","# best_rf_params = rf_study.best_params\n","# rf = RandomForestClassifier(\n","#     n_estimators=best_rf_params['n_estimators'],\n","#     max_depth=best_rf_params['max_depth'],\n","#     min_samples_split=best_rf_params['min_samples_split'],\n","#     min_samples_leaf=best_rf_params['min_samples_leaf'],\n","#     max_features=best_rf_params['max_features'],\n","#     random_state=42,\n","#     n_jobs=-1\n","# )\n","# print(\"Training Random Forest Classifier...\")\n","# rf.fit(X_train_embeddings, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pVluh03wtEdZ"},"outputs":[],"source":["# # Step 12: Evaluate the Random Forest Model\n","# print(\"Evaluating the Random Forest model...\")\n","# y_pred_rf = rf.predict(X_test_embeddings)\n","\n","# print(\"Random Forest Classification Report:\")\n","# print(classification_report(y_test, y_pred_rf, target_names=label_encoder.classes_))\n","\n","# print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n","# print(\"Random Forest Precision:\", precision_score(y_test, y_pred_rf, average='weighted'))\n","# print(\"Random Forest Recall:\", recall_score(y_test, y_pred_rf, average='weighted'))\n","# print(\"Random Forest F1 Score:\", f1_score(y_test, y_pred_rf, average='weighted'))\n","\n","# # Close the h5py files\n","# h5f.close()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNQv6+Vc6evAKQbqQaPdhm9"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}