{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T04:01:07.541764Z",
     "start_time": "2024-09-27T04:01:07.496199Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, when, isnan, isnull\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad21603dd51ffa5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T04:01:09.257595Z",
     "start_time": "2024-09-27T04:01:07.553567Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = r'C:\\miniconda3\\envs\\v\\python.exe'  # 将其替换为Python 3.10的路径\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = r'C:\\miniconda3\\envs\\v\\python.exe'  # 将其替换为Python 3.10的路径\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "            \"SentimentAnalysisBaseline\"\n",
    "        ).config(\n",
    "            \"spark.executor.memory\", \"8g\"\n",
    "        ).config(\n",
    "            \"spark.driver.memory\", \"16g\"\n",
    "        ).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdf1dbc734b85dea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T04:01:37.609497Z",
     "start_time": "2024-09-27T04:01:15.213923Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"../ggg_sg.csv\", header=True, inferSchema=True, multiLine=True, escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f247d8b0d114048",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T04:01:38.158450Z",
     "start_time": "2024-09-27T04:01:37.632503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+--------+-----------------+-----------------+---------+------+-----+-----------+--------+--------+-------+--------------------+-------------------+\n",
      "|           DateTime|                 URL|               Title|        SharingImage|LangCode|          DocTone|DomainCountryCode| Location|   Lat|  Lon|CountryCode|Adm1Code|Adm2Code|GeoType|      ContextualText|           the_geom|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------+-----------------+-----------------+---------+------+-----+-----------+--------+--------+-------+--------------------+-------------------+\n",
      "|2022-08-18 19:15:00|https://www.goal....|How to watch Manc...|https://assets.go...|     eng|0.764331210191083|               SP|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|nunez added to kl...|POINT(103.8 1.3667)|\n",
      "|2021-03-09 01:30:00|https://www.labma...|Vetter Announces ...|https://www.labma...|     eng| 3.08483290488432|               UK|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|director key acco...|POINT(103.8 1.3667)|\n",
      "|2018-05-26 13:30:00|http://www.cotswo...|Summit after all ...|http://www.cotswo...|     eng|              0.0|               UK|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|trump has said ve...|POINT(103.8 1.3667)|\n",
      "|2018-06-04 23:30:00|http://www.dailym...|Official organizi...|http://i.dailymai...|     eng|-0.15600624024961|               UK|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|south lawn follow...|POINT(103.8 1.3667)|\n",
      "|2019-06-18 14:00:00|https://www.strai...|German firm Evoni...|https://www.strai...|     eng|0.568990042674253|               SN|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|german chemical g...|POINT(103.8 1.3667)|\n",
      "|2017-05-12 06:30:00|http://www.tnp.sg...|Hard work pays of...|http://www.tnp.sg...|     eng|  2.2964509394572|               SN|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|is hard work and ...|POINT(103.8 1.3667)|\n",
      "|2020-10-02 08:45:00|https://www.asiao...|Man dies after 3 ...|https://media.asi...|     eng|-3.25379609544469|               SN|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|on sept 17 two wo...|POINT(103.8 1.3667)|\n",
      "|2018-11-30 07:30:00|https://www.busin...|It time to consid...|https://www.busin...|     eng|  1.0806916426513|               SN|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|competition betwe...|POINT(103.8 1.3667)|\n",
      "|2019-03-26 16:30:00|https://news.cisi...|Storytel launches...|                NULL|     eng| 2.27272727272727|               SW|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|is the 16th marke...|POINT(103.8 1.3667)|\n",
      "|2020-08-06 19:00:00|https://www.ig.co...|EUR / USD edges b...|https://a.c-dn.ne...|     eng|  1.0752688172043|               UK|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|may distribute in...|POINT(103.8 1.3667)|\n",
      "|2018-08-02 05:30:00|http://yourelecti...|Opteck binary opt...|                NULL|     eng| 1.15562403697997|               UK|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|at all the former...|POINT(103.8 1.3667)|\n",
      "|2017-10-31 02:00:00|http://www.asiaon...|BETADINE ( R ) 10...|http://www.asiaon...|     eng|  2.7063599458728|               SN|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|after the first m...|POINT(103.8 1.3667)|\n",
      "|2020-06-18 14:45:00|https://www.strai...|Migrant workers m...|https://www.strai...|     eng|-2.40641711229946|               SN|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|migrant workers m...|POINT(103.8 1.3667)|\n",
      "|2019-07-30 20:00:00|https://www.strai...|Pilot programme t...|https://www.strai...|     eng| 2.69299820466786|               SN|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|foreign professio...|POINT(103.8 1.3667)|\n",
      "|2018-06-14 18:15:00|http://www.asiaon...| ' Where is Singa...|http://www.asiaon...|     eng|0.441501103752759|               SN|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|east asian city s...|POINT(103.8 1.3667)|\n",
      "|2021-03-03 07:45:00|https://www.strai...|22 applications f...|https://www.strai...|     eng| 3.84615384615385|               SN|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|pass approvals an...|POINT(103.8 1.3667)|\n",
      "|2023-02-06 14:30:00|https://www.singa...|Why are wealthy I...|https://cdn.bigne...|     eng| 2.40963855421687|               SN|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|data is available...|POINT(103.8 1.3667)|\n",
      "|2017-08-07 12:30:00|http://sglinks.co...|Fintech investmen...|http://static.sgl...|     eng|0.275482093663912|               SN|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|authority of sing...|POINT(103.8 1.3667)|\n",
      "|2019-12-07 07:45:00|http://theindepen...|WP member asks On...|http://theindepen...|     eng|-2.44186046511628|               SN|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|of singapore poly...|POINT(103.8 1.3667)|\n",
      "|2021-04-09 01:45:00|https://www.manil...|Petron plans perp...|https://www.manil...|     eng|-1.63934426229508|               RP|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|that its executiv...|POINT(103.8 1.3667)|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------+-----------------+-----------------+---------+------+-----+-----------+--------+--------+-------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a081f8f0c1dd10a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T04:01:38.190998Z",
     "start_time": "2024-09-27T04:01:38.177341Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.filter(df.ContextualText.isNotNull())\n",
    "df = df.filter(df.DocTone.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a0cf6e2d0fcc816",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T04:01:38.288536Z",
     "start_time": "2024-09-27T04:01:38.269255Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"DocTone\", df[\"DocTone\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a97c1490d3dfce6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T04:01:38.474150Z",
     "start_time": "2024-09-27T04:01:38.336273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+--------+-----------+-----------------+---------+------+-----+-----------+--------+--------+-------+--------------------+-------------------+\n",
      "|           DateTime|                 URL|               Title|        SharingImage|LangCode|    DocTone|DomainCountryCode| Location|   Lat|  Lon|CountryCode|Adm1Code|Adm2Code|GeoType|      ContextualText|           the_geom|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------+-----------+-----------------+---------+------+-----+-----------+--------+--------+-------+--------------------+-------------------+\n",
      "|2022-08-18 19:15:00|https://www.goal....|How to watch Manc...|https://assets.go...|     eng|  0.7643312|               SP|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|nunez added to kl...|POINT(103.8 1.3667)|\n",
      "|2021-03-09 01:30:00|https://www.labma...|Vetter Announces ...|https://www.labma...|     eng|   3.084833|               UK|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|director key acco...|POINT(103.8 1.3667)|\n",
      "|2018-05-26 13:30:00|http://www.cotswo...|Summit after all ...|http://www.cotswo...|     eng|        0.0|               UK|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|trump has said ve...|POINT(103.8 1.3667)|\n",
      "|2018-06-04 23:30:00|http://www.dailym...|Official organizi...|http://i.dailymai...|     eng|-0.15600625|               UK|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|south lawn follow...|POINT(103.8 1.3667)|\n",
      "|2019-06-18 14:00:00|https://www.strai...|German firm Evoni...|https://www.strai...|     eng| 0.56899005|               SN|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|german chemical g...|POINT(103.8 1.3667)|\n",
      "|2017-05-12 06:30:00|http://www.tnp.sg...|Hard work pays of...|http://www.tnp.sg...|     eng|  2.2964509|               SN|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|is hard work and ...|POINT(103.8 1.3667)|\n",
      "|2020-10-02 08:45:00|https://www.asiao...|Man dies after 3 ...|https://media.asi...|     eng|  -3.253796|               SN|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|on sept 17 two wo...|POINT(103.8 1.3667)|\n",
      "|2018-11-30 07:30:00|https://www.busin...|It time to consid...|https://www.busin...|     eng|  1.0806917|               SN|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|competition betwe...|POINT(103.8 1.3667)|\n",
      "|2019-03-26 16:30:00|https://news.cisi...|Storytel launches...|                NULL|     eng|  2.2727273|               SW|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|is the 16th marke...|POINT(103.8 1.3667)|\n",
      "|2020-08-06 19:00:00|https://www.ig.co...|EUR / USD edges b...|https://a.c-dn.ne...|     eng|  1.0752689|               UK|Singapore|1.3667|103.8|         SN|      SN|    NULL|      1|may distribute in...|POINT(103.8 1.3667)|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------+-----------+-----------------+---------+------+-----+-----------+--------+--------+-------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296e08707ea46af3",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (DocTone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51c7b7febc1023be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T04:01:52.914264Z",
     "start_time": "2024-09-27T04:01:38.524406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocTone Statistics:\n",
      "Min: -28.947368621826172\n",
      "Max: 34.78260803222656\n",
      "Average: -0.03639324925228623\n",
      "Standard Deviation: 3.1345220942204715\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, min, max, avg, stddev, percentile_approx\n",
    "\n",
    "stats = df.select(\n",
    "    min(col(\"DocTone\")).alias(\"min_DocTone\"),\n",
    "    max(col(\"DocTone\")).alias(\"max_DocTone\"),\n",
    "    avg(col(\"DocTone\")).alias(\"avg_DocTone\"),\n",
    "    stddev(col(\"DocTone\")).alias(\"stddev_DocTone\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"DocTone Statistics:\")\n",
    "print(f\"Min: {stats['min_DocTone']}\")\n",
    "print(f\"Max: {stats['max_DocTone']}\")\n",
    "print(f\"Average: {stats['avg_DocTone']}\")\n",
    "print(f\"Standard Deviation: {stats['stddev_DocTone']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f0e5f9303086332",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T04:02:08.733444Z",
     "start_time": "2024-09-27T04:01:52.951499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th percentile: -28.947368621826172\n",
      "25th percentile: -2.0202019214630127\n",
      "50th percentile: 0.0\n",
      "75th percentile: 1.9910084009170532\n",
      "100th percentile: 34.78260803222656\n"
     ]
    }
   ],
   "source": [
    "# Calculate quantiles\n",
    "quantiles = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "# Calculate quantiles using approxQuantile\n",
    "quantile_values = df.approxQuantile(\"DocTone\", quantiles, relativeError=0.001)\n",
    "\n",
    "# Print quantiles\n",
    "for q, value in zip(quantiles, quantile_values):\n",
    "    print(f\"{int(q*100)}th percentile: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b7dbb8a93d3cea8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T04:02:23.581409Z",
     "start_time": "2024-09-27T04:02:08.750839Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 7) (DESKTOP-LVRFT6Q executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\miniconda3\\envs\\v\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1100, in main\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.10, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\miniconda3\\envs\\v\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1100, in main\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.10, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m sample_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocTone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m0.01\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m100000\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Collect DocTone values\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m doc_tone_values \u001b[38;5;241m=\u001b[39m \u001b[43msample_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDocTone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\v\\lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\v\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\v\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\v\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 7) (DESKTOP-LVRFT6Q executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\miniconda3\\envs\\v\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1100, in main\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.10, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\miniconda3\\envs\\v\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1100, in main\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.10, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# Sample data for visualization (100,000 samples)\n",
    "sample_df = df.select(\"DocTone\").sample(False, 0.01, seed=42).limit(100000)\n",
    "\n",
    "# Collect DocTone values\n",
    "doc_tone_values = sample_df.rdd.map(lambda row: row['DocTone']).collect()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot distribution of DocTone scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(doc_tone_values, bins=50, kde=True)\n",
    "plt.title('Distribution of DocTone Scores (Sampled Data)')\n",
    "plt.xlabel('DocTone Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cae58c5d07fe1",
   "metadata": {},
   "source": [
    "DocTone Statistics:\n",
    "\n",
    "Min: -28.947368621826172\n",
    "\n",
    "Max: 34.78260803222656\n",
    "\n",
    "Average: -0.03639324925228623\n",
    "\n",
    "Standard Deviation: 3.1345220942204715\n",
    "\n",
    "0th percentile: -28.947368621826172\n",
    "\n",
    "25th percentile: -2.0202019214630127\n",
    "\n",
    "50th percentile: 0.0\n",
    "\n",
    "75th percentile: 1.9910084009170532\n",
    "\n",
    "100th percentile: 34.78260803222656"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5623ea272d29b44f",
   "metadata": {},
   "source": [
    "# TF-IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "694f787557481223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:57:09.016232Z",
     "start_time": "2024-09-26T16:56:49.854971Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                 (0 + 1) / 1][Stage 24:>                 (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    0|2297409|\n",
      "|    1|4586529|\n",
      "|    2|2301367|\n",
      "+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create sentiment label: Positive (2), Neutral (1), Negative (0)\n",
    "def sentiment_label(score):\n",
    "    if score > 1.9910:\n",
    "        return 2\n",
    "    elif score < -2.0202:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "sentiment_udf = udf(sentiment_label, IntegerType())\n",
    "\n",
    "df = df.withColumn(\"label\", sentiment_udf(col(\"DocTone\")))\n",
    "\n",
    "label_counts = df.groupBy(\"label\").count().orderBy(\"label\")\n",
    "label_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6e555dc98847cba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:57:09.161221Z",
     "start_time": "2024-09-26T16:57:09.099983Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokenizer = Tokenizer(inputCol=\"ContextualText\", outputCol=\"words\")\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# Feature extraction (TF-IDF)\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a662ee0f2990806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T17:28:13.595306Z",
     "start_time": "2024-09-26T16:57:09.184385Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/27 01:17:56 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/09/27 01:18:50 WARN MemoryStore: Not enough space to cache rdd_164_0 in memory! (computed 17.0 MiB so far)\n",
      "24/09/27 01:18:50 WARN BlockManager: Persisting block rdd_164_0 to disk instead.\n",
      "24/09/27 01:26:14 WARN MemoryStore: Not enough space to cache rdd_164_0 in memory! (computed 419.2 MiB so far)\n",
      "24/09/27 01:26:21 WARN MemoryStore: Not enough space to cache rdd_164_0 in memory! (computed 419.2 MiB so far)\n",
      "24/09/27 01:26:31 WARN MemoryStore: Not enough space to cache rdd_164_0 in memory! (computed 419.2 MiB so far)\n",
      "24/09/27 01:26:40 WARN MemoryStore: Not enough space to cache rdd_164_0 in memory! (computed 419.2 MiB so far)\n",
      "24/09/27 01:26:52 WARN MemoryStore: Not enough space to cache rdd_164_0 in memory! (computed 419.2 MiB so far)\n",
      "24/09/27 01:27:03 WARN MemoryStore: Not enough space to cache rdd_164_0 in memory! (computed 419.2 MiB so far)\n",
      "24/09/27 01:27:15 WARN MemoryStore: Not enough space to cache rdd_164_0 in memory! (computed 419.2 MiB so far)\n",
      "24/09/27 01:27:27 WARN MemoryStore: Not enough space to cache rdd_164_0 in memory! (computed 419.2 MiB so far)\n",
      "24/09/27 01:27:38 WARN MemoryStore: Not enough space to cache rdd_164_0 in memory! (computed 419.2 MiB so far)\n",
      "24/09/27 01:27:50 WARN MemoryStore: Not enough space to cache rdd_164_0 in memory! (computed 419.2 MiB so far)\n",
      "24/09/27 01:28:02 WARN MemoryStore: Not enough space to cache rdd_164_0 in memory! (computed 419.2 MiB so far)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "pipelineModel = pipeline.fit(trainingData)\n",
    "trainingData = pipelineModel.transform(trainingData)\n",
    "testData = pipelineModel.transform(testData)\n",
    "\n",
    "# Train logistic regression model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label', maxIter=10, family='multinomial')\n",
    "lrModel = lr.fit(trainingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe67b155c0a7174d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T17:28:31.310413Z",
     "start_time": "2024-09-26T17:28:31.285501Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb8ee3afffd73dc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T17:31:20.394620Z",
     "start_time": "2024-09-26T17:28:32.686726Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.729166 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy = %g \" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "241313190236d066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T17:47:49.041369Z",
     "start_time": "2024-09-26T17:39:38.583842Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7310097965622986\n",
      "Recall: 0.7291663150352512\n",
      "F1 Score: 0.728252326514687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Detailed evaluation\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
